<!DOCTYPE HTML>
<!--
	Solid State by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ICRA LHMP 2023</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">ICRA LHMP 2023</a></h1>
						<nav>
							<a href="#menu">Menu</a>
						</nav>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<div class="inner">
							<h2>Menu</h2>
							<ul class="links">
								<li><a href="index.html">Home</a></li>
								<li><a href="program.html">Program</a></li>
								<li><a href="call-for-papers.html">Call for papers</a></li>
								<li><a href="past-events.html">Past events</a></li>
							</ul>
							<a href="#" class="close">Close</a>
						</div>
					</nav>

				<!-- Wrapper -->
				<section id="wrapper">
					<header>
						<div class="inner">
							<h2>Long-term Human Motion Prediction Workshop</h2>
							<p>Program</p>
						</div>
					</header>

					<!-- Content -->
					<div class="wrapper">
						<div class="inner">
								<p>This workshop features the talks of several high-profile invited speakers of diverse academic and industrial backgrounds and a poster session featuring the workshop proceedings.		</p>							
									<div class="table-wrapper">
										<table>
											<thead>
												<tr>
													<th>Time</th>
													<th>Speaker</th>
													<th>Topic</th>
												</tr>
											</thead>
											<tbody>
												<tr onClick='toggleRow(this)'>
													<td>9:00 - 9:15 BST</td>
													<td>Organizers</td>
													<td>Welcome and Introduction</td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>9:15 - 9:45 BST</td>
													<td><strong><a href="http://www.lambertoballan.net/">Lamberto Ballan</a></strong>, University of Padova</td>
													<td>Distilling Knowledge for Short-to-Long Term Trajectory Prediction</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>In this talk I will present our ongoing work on knowledge distillation for short-to-long term trajectory forecasting. Our approach involves training a student network to solve the long-term trajectory forecasting problem, whereas the teacher network from which the knowledge is distilled has a longer observation, and solves a short-term trajectory prediction problem by regularizing the student's predictions. Specifically, we use a teacher model to generate plausible trajectories for a shorter time horizon, and then distill the knowledge from the teacher model to a student model that solves the problem for a much higher time horizon. Our experiments show that the proposed model is beneficial for long-term forecasting, and our model achieves state-of-the-art performance on the Intersection Drone Dataset (inD) and the Stanford Drone Dataset (SDD).</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>9:45 - 10:15 BST</td>
													<td><strong><a href="http://www.alonsomora.com/">Javier Alonso-Mora, Gang Chen</a></strong>, TU Delft</td>
													<td>Particle-based Dynamic Environment Representation and Prediction for Obstacle Avoidance </td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>The design of effective motion prediction methods for dynamic objects relies heavily on the representation of the surrounding environment. Traditional approaches often employ a separated representation paradigm, utilizing static maps for modeling static objects and detecting and tracking dynamic objects separately. However, such paradigm faces certain challenges in practical applications, including trail noise in static maps and false association in multiple object tracking. In this talk, we propose a particle-based environment representation that addresses these limitations. Our representation employs particles with velocities to model both static and dynamic objects simultaneously, resulting in an ego-centric continuous occupancy map known as the Dual-structural Particle-based (DSP) map. This representation not only enhances the performance of occupancy estimation in the current time, but also enables map-wise predictions of future occupancy status. Leveraging these predictions, we propose two risk-aware motion planners to realize safe navigation in dynamic environments with pedestrians.</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>10:15 - 10:30 BST</td>
													<td><strong><a href="https://scholar.google.com/citations?user=fYCdRUwAAAAJ&hl=en">Tim Schreiter</a></strong>, University of Örebro</td>
													<td>THÖR-Magni: a new multi-modal context-rich dataset of human-robot motion</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>Social navigation is a challenging task for robots that are required to share the environment with people. Robots must model, interpret, and predict human motion and behavior, and interact and cooperate with humans in a natural and intuitive way to navigate safely and efficiently. However, existing human motion data sets are often limited in terms of tracking quality, realism, diversity, and semantic richness. This talk introduces THÖR-Magni, a novel large-scale human motion modeling and prediction dataset for social scenarios. The THÖR-Magni dataset builds on the previous THÖR dataset, which provides high-quality tracking data from motion capture, gaze trackers, and on-board robot sensors in a semantically rich environment. The THÖR-Magni dataset is an extension of THÖR with a stronger focus on sensor multi-modality, such as mobile LiDAR data and eye-tracking data being aligned with the motion data, as well as more diverse interaction scenarios, such as human-robot collaboration and guidance. The talk will also demonstrate a dashboard that allows users to do a first exploration of the data online without the need to download the data or write code.</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>10:30 - 10:45 BST</td>
													<td><strong><a href="https://www.linkedin.com/in/faris-janjos/?originalSubdomain=de">Faris Janjos</a></strong>, Bosch</td>
													<td>Unscented Autoencoder and its Application in Trajectory Prediction</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>tbd</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>10:45 - 11:15 BST</td>
													<td>Coffee break</td>
													<td></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>11:15 - 11:45 BST</td>
													<td><strong><a href="https://www.santannapisa.it/en/egidio-falotico">Egidio Falotico</a></strong>, Sant'Anna School of Advanced Studies</td>
													<td>Inferring human intentions by predicting motions: the case of robot to human handover</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>Handover tasks between humans and robots are a common interaction scenario in many industrial and household settings. However, ensuring a smooth and natural handover requires the robot to accurately predict the human's movements and intentions, including the release point of the object being handed over. 
														In this talk we present learning-based methods used to predict human motion and evaluate the engagement of the partner. Our approach is based on a deep neural network architecture that is trained on a large dataset of human hand trajectories and release points collected during handover tasks. 
														To evaluate the effectiveness of our approach, we conducted a series of experiments in which a robot interacted with human participants in handover tasks. Our results show that our approach can accurately predict both the trajectory of the recipient's hand and their intentions and release point, enabling the robot to adjust its movements in real time and ensure a smooth and effective interaction. 
														Overall, our adaptive approach improves efficiency, safety, and the overall quality of human-robot interactions. </a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>11:45 - 12:15 BST</td>
													<td><strong><a href="https://gchal.github.io/">Georgia Chalvatzaki</a></strong>, TU Darmstadt</td>
													<td>Human-centered Robot Learning for Intelligent Assistance</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>In daily lives, we need to predict and understand others’ behaviour in order to efficiently navigate through our social environment. When making predictions about what others are going to do next, we refer to their mental states, such as goals or intentions, and we are sensitive to various subtle nonverbal social cues that others display (e.g., gaze patterns). In this talk, I will present work from our lab in which we examine social signals (e.g., gaze direction, mutual gaze, means-to-goal action efficiency) in human-robot interaction. The focus of our work is on how the human brain processes such social signals. The results of our studies will be discussed in the context of design principles for social robots.</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>12:15 - 13:30 BST</td>
													<td>Lunch break</td>
													<td></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>13:30 - 14:30 BST</td>
													<td>Poster session</td>
													<td></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>14:30 - 15:00 BST</td>
													<td><strong><a href="https://www.iit.it/it/people-details/-/people/agnieszka-wykowska">Agnieszka Wykowska</a></strong>, Italian Institute of Technology</td>
													<td>Sensitivity to social signals as a way to navigate social environment</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>tbd</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>15:00 - 15:30 BST</td>
													<td><strong><a href="https://www.autonomousagents.stanford.edu/">Nick Haber</a></strong>, Stanford</td>
													<td>Trajectory prediction, social understanding, and curiosity</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>tbd</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>15:30 - 15:45 BST</td>
													<td>Coffee break</td>
													<td></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>15:45 - 16:15 BST</td>
													<td><strong><a href="https://uk.linkedin.com/in/anthony-knittel-30b180204">Anthony Knittel</a></strong>, Five AI</td>
													<td>Applied challenges of prediction for supporting an autonomous vehicle</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>Prediction is typically studied independently on datasets, while additional challenges occur when connecting prediction as part of a larger system.  We examine the use of prediction as part of a connected system along with planning and perception, in the context of supporting the Five autonomous vehicle.</a></td>
												</tr>
												<tr onClick='toggleRow(this)'>
													<td>16:15 - 16:45 BST</td>
													<td><strong><a href="https://research.nvidia.com/person/yuxiao-chen">Yuxiao Chen</a></strong>, NVIDIA</td>
													<td>How to plan with prediction: a policy planning perspective</td>
													<td class='expanded-row-content hide-row'><strong>Abstract:</strong> <a>In a typical autonomous vehicle (AV) stack, motion predictions are consumed by the planning module to generate safe and efficient motion plan for the AV. While deep learning took the field of prediction by storm and kept improving the SOTA of prediction accuracy, it is unclear how they are helping the subsequent motion plan. This talk focuses on how prediction models are used together with the downstream planning module and showed that one key factor to improving the closed-loop performance is via policy planning, that is, planning a motion policy instead of a single trajectory. Our recent works use prediction models to generate scenario trees and then plan tree-structured motion policies capable of reacting to the environment behavior. Thanks to the reactiveness, we showed that policy planning significantly outperforms the traditional benchmarks in closed-loop simulation. As expected, the increased complexity leads to higher computational cost, and we will discuss the limitations of policy planning in the talk as well.</a></td>
												</tr>
												<tr>
													<td>16:45 - 17:00 BST</td>
													<td>Organizers</td>
													<td>Discussion and conclusions</td>
												</tr>
											</tbody>
										</table>
									</div>
							</div>
						</header>
					</section>

				<!-- Footer -->
				<section id="footer">
					<div class="inner">
						<section>
							<h2 class="major">Get in touch</h2>
							<p>Please feel free to send us an <a href="mailto:Andrey.Rudenko@de.bosch.com?Subject=Workshop%20LHMP"
								target="_top">e-mail</a> , if you have any questions regarding this workshop.</p>
						</section>

						<ul class="copyright">
							<li>&copy; Andrey Rudenko and Luigi Palmieri. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</div>
				</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>


			<script>
				const toggleRow = (element) => {
				  element.getElementsByClassName('expanded-row-content')[0].classList.toggle('hide-row');
				  console.log(event);
				}
			  </script>

	</body>
</html>
